### 3.2 Результаты экспериментов

#### 3.2.1 DistilBERT

- **Конфигурация:**
  - Модель: DistilBERT-base
  - Размер батча: 32
  - Learning rate: 2e-5
  - Количество эпох: 3
  - Градиентное накопление: 4 шага
  - Weight decay: 0.01
  - Линейный разогрев learning rate: 1000 шагов

- **Метрики:**
  - Валидационная accuracy: 0.9234
  - Валидационный F1-score: 0.9228
  - Тестовая accuracy: 0.8912
  - Тестовый F1-score: 0.8905

#### 3.2.2 RoBERTa

- **Конфигурация:**
  - Модель: RoBERTa-base
  - Размер батча: 16 (уменьшен из-за большей модели)
  - Learning rate: 2e-5
  - Количество эпох: 3
  - Градиентное накопление: 4 шага
  - Weight decay: 0.01
  - Линейный разогрев learning rate: 1000 шагов

- **Метрики:**
  - Валидационная accuracy: 0.9327
  - Валидационный F1-score: 0.9319
  - Тестовая accuracy: 0.8949
  - Тестовый F1-score: 0.8935

#### 3.2.3 Сравнение моделей

| Модель | Валидационная accuracy | Валидационный F1-score | Тестовая accuracy | Тестовый F1-score |
|--------|------------------------|------------------------|-------------------|-------------------|
| DistilBERT | 0.9234 | 0.9228 | 0.8912 | 0.8905 |
| RoBERTa | 0.9327 | 0.9319 | 0.8949 | 0.8935 |

**Анализ результатов:**

1. **Производительность:**
   - RoBERTa показала лучшие результаты на валидационной выборке (+0.93% по accuracy, +0.91% по F1-score)
   - На тестовой выборке улучшение составило +0.37% по accuracy и +0.30% по F1-score
   - Обе модели демонстрируют хорошую сходимость и стабильность обучения

2. **Эффективность:**
   - DistilBERT обучается быстрее благодаря меньшему размеру модели
   - RoBERTa требует больше памяти GPU из-за большего размера модели
   - Обе модели показывают схожие паттерны обучения с плавным улучшением метрик

3. **Обобщающая способность:**
   - Небольшой разрыв между валидационными и тестовыми метриками указывает на хорошую обобщающую способность обеих моделей
   - RoBERTa демонстрирует более стабильные результаты на разных наборах данных 